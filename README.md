# Sales Data Analysis with LlamaIndex and Pandas

This project demonstrates how to use LlamaIndex to perform natural language queries on a Pandas DataFrame. It includes a Jupyter Notebook for experimentation and a Gradio application for a more interactive experience.

## Code Overview

Both the notebook and the Gradio application leverage a LlamaIndex `QueryPipeline` to process natural language queries. Here's a breakdown of the key components and steps:

1.  **Data Loading**: Sales data is loaded from a CSV file hosted on GitHub into a Pandas DataFrame.
2.  **Exploratory Analysis**: Basic checks are performed on the DataFrame to understand its structure, such as viewing the first few rows (`.head()`), getting information about data types and null values (`.info()`), and counting duplicate values.
3.  **LlamaIndex Setup**:
    *   Installs the necessary dependencies, including `llama-index` and the integration with the Groq language model.
    *   Configures `Settings.llm` to use the `llama-3.3-70b-versatile` model from Groq, using an API key.
4.  **Query Engine Creation**:
    *   A `QueryPipeline` is constructed to handle the end-to-end process of turning a user's question into a data-driven answer.
    *   **Prompt Engineering**: The pipeline uses two main prompts:
        *   A `pandas_prompt` that instructs the LLM on how to convert the natural language query into a single-line Pandas expression. This prompt is dynamically populated with the dataframe's schema and head for context.
        *   A `response_synthesis_prompt` that takes the original query, the generated Pandas expression, and its output to synthesize a natural language response for the user.
    *   **Execution Flow**: The pipeline is chained together so that the user's input query is first sent to an LLM to generate the Pandas code, which is then parsed and executed. The result, along with the original query and the generated code, is then sent to a second LLM call to create the final, user-friendly response.
5.  **Executing Queries and Synthesizing Responses**:
    *   Several questions in Portuguese are asked to the `query_engine`, such as:
        *   "What is the most used payment method?"
        *   "Which product type has the highest quantity per branch?"
        *   "Which branch has the highest revenue?"
    *   The notebook iterates over a list of questions, printing each question and the answer generated by LlamaIndex.

## How to Run

### Gradio Application (`app.py`)

The `app.py` file contains a Gradio application designed to be deployed on Hugging Face Spaces, providing a user-friendly interface for analyzing CSV files.

1.  **Upload a CSV file.**
2.  **Ask questions about the data in natural language.**
3.  **View the answers.**
4.  **Add questions and answers to a history.**
5.  **Generate a PDF report from the history.**

To run the application, you need to have the dependencies installed and your Groq API key set up. Then, run the following command in your terminal:
`python app.py`

1.  **Clone the repository (if applicable).**
2.  **Install the dependencies:**
    ```bash
    pip install pandas llama-index llama-index-experimental llama-index-llms-groq
    ```
3.  **Set up your Groq API key:**
    *   Get an API key from the [Groq console](https://console.groq.com/keys).
    *   Make sure the key is accessible in the environment where the notebook will be executed (the example uses Google Colab's `userdata`).
4.  **Run the `Llamaindex_Pandas_Query.ipynb` notebook.**

The goal is to show the simplicity and power of using LLMs to interact with and extract insights from tabular data without needing to manually write Pandas code for each query.
